#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†æ£€æŸ¥å‘é€ç»™LLMçš„è¯·æ±‚è´Ÿè½½
"""

import json
from datetime import datetime

def examine_llm_request():
    """è¯¦ç»†æ£€æŸ¥LLMè¯·æ±‚"""
    print("ğŸ” LLMè¯·æ±‚è´Ÿè½½è¯¦ç»†æ£€æŸ¥")
    print("=" * 50)
    
    # å…ˆåŠ è½½é…ç½®
    from src.common.config_utils import load_config
    load_config()
    
    from src.core.dynamic_llm_client import DynamicLLMClient
    from src.session.strict_session_manager import strict_session_manager
    
    llm_client = DynamicLLMClient()
    
    # åˆ›å»ºæµ‹è¯•ä¼šè¯
    session_id = "request_examine_" + datetime.now().strftime("%Y%m%d_%H%M%S")
    functions = [
        {
            "name": "move_to_position",
            "description": "ç§»åŠ¨åˆ°æŒ‡å®šåæ ‡ä½ç½®",
            "parameters": {
                "type": "object",
                "properties": {
                    "x": {"type": "number", "description": "Xåæ ‡"},
                    "y": {"type": "number", "description": "Yåæ ‡"}
                },
                "required": ["x", "y"]
            }
        }
    ]
    
    strict_session_manager.register_session_with_functions(
        session_id=session_id,
        client_metadata={
            "client_id": "request_examine",
            "client_type": "test",
            "client_version": "1.0.0"
        },
        functions=functions
    )
    
    print(f"âœ… åˆ›å»ºæµ‹è¯•ä¼šè¯: {session_id}")
    
    # ç”Ÿæˆè¯·æ±‚è´Ÿè½½
    user_input = "ç§»åŠ¨åˆ°(0ï¼Œ0)"
    print(f"\nğŸ“¤ ç”¨æˆ·è¾“å…¥: {user_input}")
    
    payload = llm_client.generate_function_calling_payload(
        session_id=session_id,
        user_input=user_input,
        scene_type="public",
        functions=functions
    )
    
    print(f"\nğŸ“¡ å®Œæ•´è¯·æ±‚è´Ÿè½½:")
    print(json.dumps(payload, indent=2, ensure_ascii=False))
    
    # è¯¦ç»†åˆ†æå„ä¸ªéƒ¨åˆ†
    print(f"\nğŸ“Š è´Ÿè½½ç»“æ„åˆ†æ:")
    print(f"âœ… Model: {payload.get('model')}")
    print(f"âœ… Temperature: {payload.get('temperature')}")
    print(f"âœ… Max tokens: {payload.get('max_tokens')}")
    
    # æ£€æŸ¥æ¶ˆæ¯ç»“æ„
    messages = payload.get('messages', [])
    print(f"\nğŸ’¬ æ¶ˆæ¯ç»“æ„ ({len(messages)} æ¡):")
    for i, msg in enumerate(messages):
        print(f"  æ¶ˆæ¯ {i+1}:")
        print(f"    Role: {msg.get('role')}")
        print(f"    Content: {repr(msg.get('content')[:100])}{'...' if len(msg.get('content', '')) > 100 else ''}")
    
    # æ£€æŸ¥å‡½æ•°è°ƒç”¨é…ç½®
    print(f"\nğŸ”§ å‡½æ•°è°ƒç”¨é…ç½®:")
    has_functions = 'functions' in payload
    has_function_call = 'function_call' in payload
    
    print(f"âœ… åŒ…å«functionså­—æ®µ: {has_functions}")
    print(f"âœ… åŒ…å«function_callå­—æ®µ: {has_function_call}")
    
    if has_functions:
        functions_list = payload['functions']
        print(f"ğŸ“Š å‡½æ•°å®šä¹‰æ•°é‡: {len(functions_list)}")
        for i, func in enumerate(functions_list):
            print(f"  å‡½æ•° {i+1}:")
            print(f"    Name: {func.get('name')}")
            print(f"    Description: {func.get('description')}")
            params = func.get('parameters', {})
            print(f"    å‚æ•°å±æ€§æ•°: {len(params.get('properties', {}))}")
            print(f"    å¿…éœ€å‚æ•°: {params.get('required', [])}")
    
    if has_function_call:
        print(f"ğŸ¯ Function callæ¨¡å¼: {payload['function_call']}")
    else:
        print(f"âš ï¸  ç¼ºå°‘function_callå­—æ®µ")
        
    # æ£€æŸ¥LLMé…ç½®
    print(f"\nâš™ï¸  LLMé…ç½®æ£€æŸ¥:")
    config = llm_client.__dict__
    print(f"âœ… Base URL: {config.get('base_url')}")
    print(f"âœ… Model: {config.get('model')}")
    print(f"âœ… Timeout: {config.get('timeout')}ç§’")
    
    # éªŒè¯å‡½æ•°å®šä¹‰æ ¼å¼
    print(f"\nğŸ“‹ å‡½æ•°å®šä¹‰éªŒè¯:")
    from src.models.function_calling_models import is_valid_openai_function
    
    for i, func_def in enumerate(functions):
        is_valid = is_valid_openai_function(func_def)
        status = "âœ…" if is_valid else "âŒ"
        print(f"  {status} å‡½æ•° {i+1} ({func_def.get('name', 'unknown')}): {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")
        
        if not is_valid:
            print(f"     é—®é¢˜: {func_def}")

if __name__ == "__main__":
    examine_llm_request()