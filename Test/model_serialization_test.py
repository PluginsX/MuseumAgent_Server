#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥æµ‹è¯•Pydanticæ¨¡å‹åºåˆ—åŒ–æ•ˆæœ
"""

import sys
import os
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ç§»é™¤StandardCommandå¯¼å…¥ï¼Œè¯¥æ¨¡å‹å·²è¢«åºŸå¼ƒ
# from src.models.response_models import StandardCommand

def test_model_serialization():
    """æµ‹è¯•æ¨¡å‹åºåˆ—åŒ–æ•ˆæœ"""
    print("=" * 80)
    print("ğŸ§ª Pydanticæ¨¡å‹åºåˆ—åŒ–æµ‹è¯•")
    print("=" * 80)
    
    # æ¨¡æ‹ŸLLMåŸå§‹å“åº”æ•°æ®
    llm_raw_response = {
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "message": {
                    "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
                    "role": "assistant"
                }
            }
        ],
        "created": 1770217100,
        "id": "chatcmpl-test-123",
        "model": "qwen-turbo",
        "object": "chat.completion",
        "usage": {
            "completion_tokens": 8,
            "prompt_tokens": 332,
            "prompt_tokens_details": {
                "cached_tokens": 0
            },
            "total_tokens": 340
        }
    }
    
    print("åŸå§‹LLMå“åº”:")
    print(json.dumps(llm_raw_response, indent=2, ensure_ascii=False))
    
    # 1. ç›´æ¥ä½¿ç”¨å­—å…¸ï¼ˆæ¨¡æ‹Ÿå½“å‰è¡Œä¸ºï¼‰
    print("\n1. ç›´æ¥ä½¿ç”¨å­—å…¸åºåˆ—åŒ–:")
    print("-" * 40)
    direct_dict = llm_raw_response.copy()
    print("åºåˆ—åŒ–ç»“æœ:")
    print(json.dumps(direct_dict, indent=2, ensure_ascii=False))
    print(f"å­—æ®µæ•°: {len(direct_dict)}")
    
    # 2. ä½¿ç”¨StandardCommandæ¨¡å‹ï¼ˆä¿®å¤å‰ï¼‰
    print("\n2. ä½¿ç”¨StandardCommandæ¨¡å‹ï¼ˆä¿®å¤å‰ï¼‰:")
    print("-" * 40)
    # ä¸´æ—¶ä¿®æ”¹æ¨¡å‹é…ç½®æ¥æ¨¡æ‹Ÿä¿®å¤å‰çš„è¡Œä¸º
    original_config = StandardCommand.Config
    StandardCommand.Config.exclude_none = False
    
    try:
        model_instance = StandardCommand(**llm_raw_response)
        serialized = model_instance.model_dump()
        print("åºåˆ—åŒ–ç»“æœ:")
        print(json.dumps(serialized, indent=2, ensure_ascii=False))
        print(f"å­—æ®µæ•°: {len(serialized)}")
        
        # æ˜¾ç¤ºå¤šä½™çš„å­—æ®µ
        extra_fields = set(serialized.keys()) - set(llm_raw_response.keys())
        if extra_fields:
            print(f"å¤šä½™å­—æ®µ: {sorted(extra_fields)}")
    finally:
        # æ¢å¤åŸå§‹é…ç½®
        StandardCommand.Config.exclude_none = True
    
    # 3. ä½¿ç”¨StandardCommandæ¨¡å‹ï¼ˆä¿®å¤åï¼‰
    print("\n3. ä½¿ç”¨StandardCommandæ¨¡å‹ï¼ˆä¿®å¤åï¼‰:")
    print("-" * 40)
    model_instance = StandardCommand(**llm_raw_response)
    serialized = model_instance.model_dump()
    print("åºåˆ—åŒ–ç»“æœ:")
    print(json.dumps(serialized, indent=2, ensure_ascii=False))
    print(f"å­—æ®µæ•°: {len(serialized)}")
    
    # éªŒè¯æ˜¯å¦ä¸åŸå§‹æ•°æ®ä¸€è‡´
    if set(serialized.keys()) == set(llm_raw_response.keys()):
        print("âœ… åºåˆ—åŒ–ç»“æœä¸åŸå§‹æ•°æ®å­—æ®µå®Œå…¨ä¸€è‡´")
    else:
        print("âŒ åºåˆ—åŒ–ç»“æœä¸åŸå§‹æ•°æ®å­—æ®µä¸ä¸€è‡´")
        missing = set(llm_raw_response.keys()) - set(serialized.keys())
        extra = set(serialized.keys()) - set(llm_raw_response.keys())
        if missing:
            print(f"ç¼ºå¤±å­—æ®µ: {missing}")
        if extra:
            print(f"å¤šä½™å­—æ®µ: {extra}")

def demonstrate_fix_effect():
    """æ¼”ç¤ºä¿®å¤æ•ˆæœ"""
    print("\n" + "=" * 80)
    print("âœ¨ ä¿®å¤æ•ˆæœæ¼”ç¤º")
    print("=" * 80)
    
    # ç”¨æˆ·çœ‹åˆ°çš„é—®é¢˜å“åº”
    problematic = {
        "artifact_id": None,
        "artifact_name": None,
        "operation": None,
        "operation_params": None,
        "keywords": None,
        "tips": None,
        "response": None,
        "command": None,
        "parameters": None,
        "type": None,
        "format": None,
        "timestamp": None,
        "session_id": None,
        "processing_mode": None,
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "message": {
                    "content": "æˆ‘ä¼šå¸®åŠ©æ‚¨äº†è§£å’Œæ¢ç´¢å„ç§æ–‡ç‰©çš„ä¿¡æ¯...",
                    "role": "assistant"
                }
            }
        ],
        "created": 1770217100,
        "id": "chatcmpl-problematic",
        "model": "qwen-turbo",
        "object": "chat.completion",
        "usage": {
            "completion_tokens": 35,
            "prompt_tokens": 333,
            "prompt_tokens_details": {"cached_tokens": 0},
            "total_tokens": 368
        }
    }
    
    # ä¿®å¤åçš„ç†æƒ³å“åº”
    ideal = {
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "message": {
                    "content": "æˆ‘ä¼šå¸®åŠ©æ‚¨äº†è§£å’Œæ¢ç´¢å„ç§æ–‡ç‰©çš„ä¿¡æ¯...",
                    "role": "assistant"
                }
            }
        ],
        "created": 1770217100,
        "id": "chatcmpl-ideal",
        "model": "qwen-turbo",
        "object": "chat.completion",
        "usage": {
            "completion_tokens": 35,
            "prompt_tokens": 333,
            "prompt_tokens_details": {"cached_tokens": 0},
            "total_tokens": 368
        }
    }
    
    print("é—®é¢˜å“åº”å­—æ®µæ•°:", len(problematic))
    print("ç†æƒ³å“åº”å­—æ®µæ•°:", len(ideal))
    print("å‡å°‘å­—æ®µæ•°:", len(problematic) - len(ideal))
    
    print(f"\né—®é¢˜å“åº”å¤§å°: {len(json.dumps(problematic))} å­—ç¬¦")
    print(f"ç†æƒ³å“åº”å¤§å°: {len(json.dumps(ideal))} å­—ç¬¦")
    print(f"å‡å°‘å¤§å°: {len(json.dumps(problematic)) - len(json.dumps(ideal))} å­—ç¬¦")
    
    # æ‰¾å‡ºè¢«ç§»é™¤çš„å­—æ®µ
    removed_fields = set(problematic.keys()) - set(ideal.keys())
    print(f"\nè¢«ç§»é™¤çš„å­—æ®µ ({len(removed_fields)}ä¸ª):")
    for field in sorted(removed_fields):
        print(f"  - {field}")

if __name__ == "__main__":
    print("ğŸš€ Pydanticæ¨¡å‹åºåˆ—åŒ–ä¿®å¤éªŒè¯")
    print()
    
    # æµ‹è¯•æ¨¡å‹åºåˆ—åŒ–
    test_model_serialization()
    
    # æ¼”ç¤ºä¿®å¤æ•ˆæœ
    demonstrate_fix_effect()
    
    print(f"\nğŸ æµ‹è¯•å®Œæˆ")