#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡½æ•°è°ƒç”¨ä¿¡æ¯ä¸¢å¤±é—®é¢˜è°ƒè¯•è„šæœ¬
è¿½è¸ªä»LLMå“åº”åˆ°å®¢æˆ·ç«¯æ•°æ®çš„å®Œæ•´è½¬æ¢æµç¨‹
"""

import json
import sys
import os
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from src.common.config_utils import load_config
from src.core.dynamic_llm_client import DynamicLLMClient
from src.session.strict_session_manager import strict_session_manager
from src.core.command_generator import CommandGenerator

# åŠ è½½é…ç½®
load_config()

def debug_function_call_transformation():
    """è°ƒè¯•å‡½æ•°è°ƒç”¨ä¿¡æ¯çš„å®Œæ•´è½¬æ¢æµç¨‹"""
    print("=" * 80)
    print("ğŸ” å‡½æ•°è°ƒç”¨ä¿¡æ¯è½¬æ¢æµç¨‹è°ƒè¯•")
    print("=" * 80)
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸ”§ æ­¥éª¤1: åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶")
    print("-" * 40)
    llm_client = DynamicLLMClient()
    command_generator = CommandGenerator()
    
    # åˆ›å»ºæµ‹è¯•ä¼šè¯
    session_id = "debug-session-func-call"
    functions = [
        {
            "name": "introduce_artifact",
            "description": "ä»‹ç»æŒ‡å®šæ–‡ç‰©çš„è¯¦ç»†ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "artifact_name": {
                        "type": "string",
                        "description": "æ–‡ç‰©åç§°"
                    }
                },
                "required": ["artifact_name"]
            }
        }
    ]
    
    # æ³¨å†Œä¼šè¯
    strict_session_manager.register_session_with_functions(
        session_id=session_id,
        client_metadata={
            "client_id": "debug_client",
            "client_type": "debug",
            "client_version": "1.0.0"
        },
        functions=functions
    )
    
    print(f"âœ… å·²æ³¨å†Œæµ‹è¯•ä¼šè¯: {session_id}")
    print(f"âœ… å·²æ³¨å†Œå‡½æ•°æ•°é‡: {len(functions)}")
    
    # 2. æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥
    print("\nğŸ’¬ æ­¥éª¤2: æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥")
    print("-" * 40)
    user_input = "è¯·ä»‹ç»èŸ é¾™ç›–ç½è¿™ä»¶æ–‡ç‰©"
    scene_type = "study"
    
    print(f"ç”¨æˆ·è¾“å…¥: {user_input}")
    print(f"åœºæ™¯ç±»å‹: {scene_type}")
    
    # 3. ç”Ÿæˆå‡½æ•°è°ƒç”¨è´Ÿè½½
    print("\nğŸ“¤ æ­¥éª¤3: ç”ŸæˆLLMè¯·æ±‚è´Ÿè½½")
    print("-" * 40)
    payload = llm_client.generate_function_calling_payload(
        session_id=session_id,
        user_input=user_input,
        scene_type=scene_type,
        rag_instruction="",
        functions=functions
    )
    
    print("å‘é€ç»™LLMçš„è¯·æ±‚è´Ÿè½½:")
    print(json.dumps(payload, indent=2, ensure_ascii=False))
    
    # 4. æ¨¡æ‹ŸLLMå“åº”ï¼ˆåŒ…å«å‡½æ•°è°ƒç”¨ï¼‰
    print("\nğŸ“¥ æ­¥éª¤4: æ¨¡æ‹ŸLLMåŸå§‹å“åº”")
    print("-" * 40)
    mock_llm_response = {
        "id": "chatcmpl-debug001",
        "object": "chat.completion",
        "created": 1707064900,
        "model": "qwen-turbo",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†ä»‹ç»èŸ é¾™ç›–ç½è¿™ä»¶çè´µæ–‡ç‰©ã€‚",
                    "function_call": {
                        "name": "introduce_artifact",
                        "arguments": "{\"artifact_name\": \"èŸ é¾™ç›–ç½\"}"
                    }
                },
                "finish_reason": "function_call"
            }
        ],
        "usage": {
            "prompt_tokens": 85,
            "completion_tokens": 42,
            "total_tokens": 127
        }
    }
    
    print("LLMåŸå§‹å“åº”:")
    print(json.dumps(mock_llm_response, indent=2, ensure_ascii=False))
    
    # 5. è§£æLLMå“åº”
    print("\nğŸ”„ æ­¥éª¤5: è§£æLLMå“åº”")
    print("-" * 40)
    parsed_result = llm_client.parse_function_call_response(mock_llm_response)
    
    print("è§£æåçš„ä¸­é—´ç»“æœ:")
    print(json.dumps(parsed_result, indent=2, ensure_ascii=False))
    
    # 6. é€šè¿‡å‘½ä»¤ç”Ÿæˆå™¨å¤„ç†
    print("\nâš™ï¸ æ­¥éª¤6: é€šè¿‡CommandGeneratorå¤„ç†")
    print("-" * 40)
    try:
        command_result = command_generator.generate_standard_command(
            user_input=user_input,
            scene_type=scene_type,
            session_id=session_id
        )
        
        print("CommandGeneratorå¤„ç†ç»“æœ:")
        print(json.dumps(command_result, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"âŒ CommandGeneratorå¤„ç†å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 7. æœ€ç»ˆAPIå“åº”æ ¼å¼
    print("\nğŸŒ æ­¥éª¤7: æœ€ç»ˆAPIå“åº”æ ¼å¼")
    print("-" * 40)
    from src.common.response_utils import success_response
    
    final_response = success_response(data=command_result)
    print("å‘é€ç»™å®¢æˆ·ç«¯çš„æœ€ç»ˆå“åº”:")
    print(json.dumps(final_response, indent=2, ensure_ascii=False))
    
    # 8. æ•°æ®æµå¯¹æ¯”åˆ†æ
    print("\nğŸ“Š æ­¥éª¤8: æ•°æ®æµå¯¹æ¯”åˆ†æ")
    print("-" * 40)
    
    print("LLMåŸå§‹å“åº”ä¸­çš„å‡½æ•°è°ƒç”¨ä¿¡æ¯:")
    llm_function_call = mock_llm_response["choices"][0]["message"].get("function_call")
    if llm_function_call:
        print(f"  å‡½æ•°å: {llm_function_call.get('name')}")
        print(f"  å‚æ•°: {llm_function_call.get('arguments')}")
        print(f"  å¯¹è¯å†…å®¹: {mock_llm_response['choices'][0]['message'].get('content')}")
    
    print("\næœ€ç»ˆå®¢æˆ·ç«¯æ¥æ”¶åˆ°çš„æ•°æ®:")
    client_data = final_response.get("data", {})
    print(f"  command: {client_data.get('command')}")
    print(f"  parameters: {client_data.get('parameters')}")
    print(f"  response: {client_data.get('response')}")
    print(f"  type: {client_data.get('type')}")
    
    # 9. é—®é¢˜è¯Šæ–­
    print("\nğŸ” æ­¥éª¤9: é—®é¢˜è¯Šæ–­")
    print("-" * 40)
    
    # æ£€æŸ¥å‡½æ•°è°ƒç”¨ä¿¡æ¯æ˜¯å¦å®Œæ•´ä¼ é€’
    if llm_function_call and client_data.get('command'):
        if llm_function_call.get('name') == client_data.get('command'):
            print("âœ… å‡½æ•°åä¼ é€’æ­£ç¡®")
        else:
            print("âŒ å‡½æ•°åä¼ é€’é”™è¯¯")
            
        llm_args = llm_function_call.get('arguments', '{}')
        client_params = client_data.get('parameters', {})
        try:
            llm_parsed_args = json.loads(llm_args)
            if llm_parsed_args == client_params:
                print("âœ… å‡½æ•°å‚æ•°ä¼ é€’æ­£ç¡®")
            else:
                print("âŒ å‡½æ•°å‚æ•°ä¼ é€’é”™è¯¯")
                print(f"  LLMåŸå§‹å‚æ•°: {llm_parsed_args}")
                print(f"  å®¢æˆ·ç«¯æ¥æ”¶å‚æ•°: {client_params}")
        except json.JSONDecodeError:
            print("âŒ LLMå‚æ•°JSONè§£æå¤±è´¥")
    else:
        print("âš ï¸  å‡½æ•°è°ƒç”¨ä¿¡æ¯ç¼ºå¤±")
    
    # æ£€æŸ¥å¯¹è¯å†…å®¹
    llm_content = mock_llm_response["choices"][0]["message"].get("content", "")
    client_response = client_data.get("response", "")
    if llm_content == client_response:
        print("âœ… å¯¹è¯å†…å®¹ä¼ é€’æ­£ç¡®")
    else:
        print("âŒ å¯¹è¯å†…å®¹ä¼ é€’é”™è¯¯")
        print(f"  LLMåŸå§‹å†…å®¹: '{llm_content}'")
        print(f"  å®¢æˆ·ç«¯æ¥æ”¶å†…å®¹: '{client_response}'")
    
    print("\n" + "=" * 80)
    print("ğŸ¯ è°ƒè¯•å®Œæˆ")
    print("=" * 80)

if __name__ == "__main__":
    debug_function_call_transformation()