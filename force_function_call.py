#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºåˆ¶å‡½æ•°è°ƒç”¨æµ‹è¯• - ä¿®æ”¹æç¤ºè¯å’Œè¾“å…¥æ¥å¼ºåˆ¶è§¦å‘å‡½æ•°è°ƒç”¨
"""

import json
from datetime import datetime

def force_function_call_test():
    """å¼ºåˆ¶å‡½æ•°è°ƒç”¨æµ‹è¯•"""
    print("ğŸ’ª å¼ºåˆ¶å‡½æ•°è°ƒç”¨æµ‹è¯•")
    print("=" * 40)
    
    # å…ˆåŠ è½½é…ç½®
    from src.common.config_utils import load_config
    load_config()
    
    from src.core.dynamic_llm_client import DynamicLLMClient
    from src.session.strict_session_manager import strict_session_manager
    
    llm_client = DynamicLLMClient()
    
    # åˆ›å»ºæµ‹è¯•ä¼šè¯
    session_id = "force_call_" + datetime.now().strftime("%Y%m%d_%H%M%S")
    functions = [
        {
            "name": "move_to_position",
            "description": "ç§»åŠ¨åˆ°æŒ‡å®šåæ ‡ä½ç½®",
            "parameters": {
                "type": "object",
                "properties": {
                    "x": {"type": "number", "description": "Xåæ ‡"},
                    "y": {"type": "number", "description": "Yåæ ‡"}
                },
                "required": ["x", "y"]
            }
        }
    ]
    
    strict_session_manager.register_session_with_functions(
        session_id=session_id,
        client_metadata={
            "client_id": "force_call",
            "client_type": "test",
            "client_version": "1.0.0"
        },
        functions=functions
    )
    
    print(f"âœ… åˆ›å»ºæµ‹è¯•ä¼šè¯: {session_id}")
    
    # æµ‹è¯•ä¸åŒçš„ç”¨æˆ·è¾“å…¥å’Œæç¤ºè¯ç»„åˆ
    test_cases = [
        {
            "name": "æ˜ç¡®æŒ‡ä»¤å¼è¾“å…¥",
            "user_input": "è¯·è°ƒç”¨move_to_positionå‡½æ•°ï¼Œå°†åæ ‡è®¾ç½®ä¸ºx=0,y=0",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªå‡½æ•°è°ƒç”¨åŠ©æ‰‹ã€‚å½“ç”¨æˆ·è¦æ±‚æ‰§è¡Œå…·ä½“åŠ¨ä½œæ—¶ï¼Œå¿…é¡»è°ƒç”¨ç›¸åº”çš„å‡½æ•°ã€‚è¯·åˆ†æç”¨æˆ·éœ€æ±‚å¹¶è°ƒç”¨åˆé€‚çš„å‡½æ•°ã€‚"
        },
        {
            "name": "å¼ºæŒ‡ç¤ºæç¤ºè¯",
            "user_input": "ç§»åŠ¨åˆ°åæ ‡(0,0)",
            "system_prompt": "ä½ å¿…é¡»è°ƒç”¨move_to_positionå‡½æ•°æ¥å¤„ç†åæ ‡ç§»åŠ¨è¯·æ±‚ã€‚è¿™æ˜¯å¼ºåˆ¶è¦æ±‚ã€‚"
        },
        {
            "name": "æ··åˆæ¨¡å¼",
            "user_input": "æˆ‘ç°åœ¨éœ€è¦ç§»åŠ¨åˆ°(0,0)è¿™ä¸ªä½ç½®ï¼Œè¯·æ‰§è¡Œç§»åŠ¨æ“ä½œ",
            "system_prompt": "å¯¹äºç§»åŠ¨è¯·æ±‚ï¼Œä½ å¿…é¡»è°ƒç”¨move_to_positionå‡½æ•°ã€‚å…ˆè§£é‡Šä½ è¦åšä»€ä¹ˆï¼Œç„¶åè°ƒç”¨å‡½æ•°ã€‚"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['name']} ---")
        print(f"ç”¨æˆ·è¾“å…¥: {test_case['user_input']}")
        
        # æ„å»ºè‡ªå®šä¹‰payload
        messages = [
            {"role": "system", "content": test_case['system_prompt']},
            {"role": "user", "content": f"åœºæ™¯ï¼špublic\nç”¨æˆ·è¾“å…¥ï¼š{test_case['user_input']}"}
        ]
        
        payload = {
            "model": llm_client.model,
            "messages": messages,
            "temperature": 0.1,  # æ›´ä½çš„æ¸©åº¦å¢åŠ ç¡®å®šæ€§
            "max_tokens": 512,
            "top_p": 0.1,
            "functions": functions,
            "function_call": "auto"
        }
        
        print(f"ğŸ“¡ å‘é€è¯·æ±‚...")
        
        try:
            response = llm_client._chat_completions_with_functions(payload)
            
            # åˆ†æå“åº”
            choices = response.get("choices", [])
            if choices:
                message = choices[0].get("message", {})
                function_call = message.get("function_call")
                content = message.get("content", "")
                
                print(f"ğŸ“¥ å“åº”åˆ†æ:")
                print(f"   å¯¹è¯å†…å®¹: {repr(content)}")
                print(f"   è§¦å‘å‡½æ•°è°ƒç”¨: {function_call is not None}")
                
                if function_call:
                    print(f"   âœ… æˆåŠŸè§¦å‘å‡½æ•°è°ƒç”¨!")
                    print(f"   å‡½æ•°å: {function_call.get('name')}")
                    print(f"   å‚æ•°: {function_call.get('arguments')}")
                else:
                    print(f"   âŒ ä»æœªè§¦å‘å‡½æ•°è°ƒç”¨")
                    
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def test_with_different_models():
    """æµ‹è¯•ä¸åŒæ¨¡å‹çš„è¡Œä¸º"""
    print("\nğŸ”¬ ä¸åŒæ¨¡å‹è¡Œä¸ºæµ‹è¯•")
    print("=" * 40)
    
    # å…ˆåŠ è½½é…ç½®
    from src.common.config_utils import load_config, get_global_config
    load_config()
    
    config = get_global_config()
    llm_config = config.get("llm", {})
    
    print(f"å½“å‰æ¨¡å‹: {llm_config.get('model', 'unknown')}")
    print(f"Base URL: {llm_config.get('base_url', 'unknown')}")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"1. å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼ˆå¦‚qwen-plus, qwen-maxç­‰ï¼‰")
    print(f"2. è°ƒæ•´temperatureå‚æ•°ï¼ˆæ›´ä½çš„å€¼å¢åŠ ç¡®å®šæ€§ï¼‰")
    print(f"3. ä¿®æ”¹æç¤ºè¯ç­–ç•¥ï¼Œæ›´åŠ æ˜ç¡®åœ°æŒ‡ç¤ºå‡½æ•°è°ƒç”¨")
    print(f"4. è€ƒè™‘ä½¿ç”¨'function_call': 'required'å¼ºåˆ¶æ¨¡å¼")

if __name__ == "__main__":
    force_function_call_test()
    test_with_different_models()