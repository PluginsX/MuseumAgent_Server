# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€STTæœåŠ¡æ¨¡å—
ç”¨äºå¤„ç†è¯­éŸ³è½¬æ–‡æœ¬åŠŸèƒ½
ä½¿ç”¨é˜¿é‡Œäº‘DashScope SDKè¿›è¡Œè¯­éŸ³è¯†åˆ«
"""
import json
import asyncio
import base64
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime

# å¯¼å…¥é˜¿é‡Œäº‘DashScope SDK
import dashscope
from dashscope.audio.asr import Recognition, RecognitionCallback

# åˆ›å»ºä¸€ä¸ªç®€å•çš„å›è°ƒç±»
class SimpleRecognitionCallback(RecognitionCallback):
    def __init__(self):
        super().__init__()
        self.results = []

    def on_open(self):
        pass

    def on_error(self, message=None):
        pass

    def on_close(self):
        pass

    def on_event(self, message):
        self.results.append(message)

from src.common.enhanced_logger import get_enhanced_logger, Module
from src.common.config_utils import get_global_config


class UnifiedSTTService:
    """ç»Ÿä¸€STTæœåŠ¡ - é›†æˆæµå¼å’Œéæµå¼STTåŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–STTæœåŠ¡"""
        self.logger = get_enhanced_logger()
        # å»¶è¿ŸåŠ è½½é…ç½®ï¼Œç›´åˆ°å®é™…ä½¿ç”¨æ—¶
        self._config = None
        self._stt_config = None
        self.stt_base_url = None
        self.stt_api_key = None
        self.stt_model = None

    def _ensure_config_loaded(self):
        """ç¡®ä¿é…ç½®å·²åŠ è½½"""
        if self._config is None:
            self._config = get_global_config()
            self._stt_config = self._config.get("stt", {})
            
            # STTå®¢æˆ·ç«¯é…ç½®
            self.stt_api_key = self._stt_config.get("api_key", "")
            self.stt_model = self._stt_config.get("model", "paraformer-realtime-v2")
            
            # è®¾ç½®DashScope APIå¯†é’¥
            if self.stt_api_key:
                dashscope.api_key = self.stt_api_key
            else:
                raise RuntimeError("STT æœªé…ç½® api_keyï¼Œè¯·åœ¨ config.json ä¸­è®¾ç½®")
    
    def _detect_audio_format(self, audio_data: bytes) -> tuple:
        """
        è‡ªåŠ¨æ£€æµ‹éŸ³é¢‘æ ¼å¼
        
        Args:
            audio_data: éŸ³é¢‘äºŒè¿›åˆ¶æ•°æ®
            
        Returns:
            (format_name, file_suffix) å…ƒç»„
        """
        if len(audio_data) < 4:
            # æ•°æ®å¤ªçŸ­ï¼Œé»˜è®¤ä¸ºmp3
            return ('mp3', '.mp3')
        
        # æ£€æŸ¥æ–‡ä»¶å¤´é­”æ•°
        header = audio_data[:4]
        
        # WebMæ ¼å¼ (EBML header: 0x1A 0x45 0xDF 0xA3)
        if header == b'\x1a\x45\xdf\xa3':
            self.logger.stt.info("Detected WebM format")
            return ('webm', '.webm')
        
        # MP3æ ¼å¼ (ID3 tag: 'ID3' or MPEG frame sync: 0xFF 0xFB/0xFA)
        if header[:3] == b'ID3' or (header[0] == 0xFF and (header[1] & 0xE0) == 0xE0):
            self.logger.stt.info("Detected MP3 format")
            return ('mp3', '.mp3')
        
        # WAVæ ¼å¼ (RIFF header: 'RIFF')
        if header[:4] == b'RIFF':
            self.logger.stt.info("Detected WAV format")
            return ('wav', '.wav')
        
        # OGGæ ¼å¼ (OggS header)
        if header[:4] == b'OggS':
            self.logger.stt.info("Detected OGG format")
            return ('ogg', '.ogg')
        
        # M4A/AACæ ¼å¼ (ftyp box)
        if len(audio_data) >= 8 and audio_data[4:8] == b'ftyp':
            self.logger.stt.info("Detected M4A/AAC format")
            return ('m4a', '.m4a')
        
        # é»˜è®¤ä¸ºmp3
        self.logger.stt.warning(f"Unknown audio format, header: {header.hex()}, defaulting to mp3")
        return ('mp3', '.mp3')
    
    def _convert_pcm_to_wav(self, pcm_data: bytes) -> bytes:
        """
        å°†è£¸PCMæ•°æ®è½¬æ¢ä¸ºWAVæ ¼å¼
        
        Args:
            pcm_data: è£¸PCMæ•°æ®ï¼ˆ16bit, 16kHz, å•å£°é“ï¼‰
            
        Returns:
            WAVæ ¼å¼çš„éŸ³é¢‘æ•°æ®
        """
        import struct
        
        # WAVæ–‡ä»¶å‚æ•°
        sample_rate = 16000
        num_channels = 1
        bits_per_sample = 16
        byte_rate = sample_rate * num_channels * bits_per_sample // 8
        block_align = num_channels * bits_per_sample // 8
        data_size = len(pcm_data)
        file_size = 36 + data_size
        
        # æ„å»ºWAVæ–‡ä»¶å¤´ï¼ˆ44å­—èŠ‚ï¼‰
        wav_header = struct.pack(
            '<4sI4s4sIHHIIHH4sI',
            b'RIFF',           # ChunkID
            file_size,         # ChunkSize
            b'WAVE',           # Format
            b'fmt ',           # Subchunk1ID
            16,                # Subchunk1Size (PCM)
            1,                 # AudioFormat (PCM)
            num_channels,      # NumChannels
            sample_rate,       # SampleRate
            byte_rate,         # ByteRate
            block_align,       # BlockAlign
            bits_per_sample,   # BitsPerSample
            b'data',           # Subchunk2ID
            data_size          # Subchunk2Size
        )
        
        self.logger.stt.info(f"Converted PCM to WAV: {len(pcm_data)} bytes PCM -> {len(wav_header) + len(pcm_data)} bytes WAV")
        
        return wav_header + pcm_data
    
    async def recognize_audio(self, audio_data: bytes, audio_format_hint: str = None) -> str:
        """
        è¯†åˆ«éŸ³é¢‘ä¸ºæ–‡æœ¬ï¼ˆéæµå¼ï¼‰
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            audio_format_hint: éŸ³é¢‘æ ¼å¼æç¤ºï¼ˆå¯é€‰ï¼Œå¦‚'pcm', 'wav', 'mp3'ç­‰ï¼‰
            
        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æœ¬
        """
        if not audio_data:
            return ""
        
        # ç¡®ä¿é…ç½®å·²åŠ è½½
        self._ensure_config_loaded()
        
        # ä¼˜å…ˆä½¿ç”¨æ ¼å¼æç¤ºï¼Œå¦åˆ™è‡ªåŠ¨æ£€æµ‹
        if audio_format_hint:
            audio_format = audio_format_hint.lower()
            file_suffix = f'.{audio_format}'
            self.logger.stt.info(f"Using provided audio format hint: {audio_format}")
        else:
            audio_format, file_suffix = self._detect_audio_format(audio_data)
        
        self.logger.stt.info("Starting speech recognition", {
                'audio_size': len(audio_data),
                'format': audio_format,
                'file_suffix': file_suffix
            })
        
        # ğŸ” è®¡ç®—éŸ³é¢‘æ—¶é•¿
        if audio_format == 'pcm':
            # PCM: 16bit = 2 bytes/sample, 16kHz = 16000 samples/second
            duration_seconds = len(audio_data) / (16000 * 2)
            self.logger.stt.info(f"PCM audio duration: {duration_seconds:.3f} seconds")
            
            # âš ï¸ æ£€æŸ¥æ—¶é•¿æ˜¯å¦å¤ªçŸ­
            if duration_seconds < 0.5:
                self.logger.stt.info(f"Audio duration too short: {duration_seconds:.3f}s < 0.5s, may cause recognition failure")
        
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥éŸ³é¢‘æ•°æ®çš„å®é™…å†…å®¹
        if len(audio_data) >= 16:
            header_hex = audio_data[:16].hex()
            self.logger.stt.info(f"Audio data header (first 16 bytes): {header_hex}")
            
            # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯é›¶ï¼ˆé™éŸ³ï¼‰
            non_zero_count = sum(1 for b in audio_data if b != 0)
            zero_ratio = (len(audio_data) - non_zero_count) / len(audio_data)
            self.logger.stt.info(f"Audio data analysis: non_zero={non_zero_count}/{len(audio_data)}, zero_ratio={zero_ratio:.2%}")
            
            # å¦‚æœæ˜¯PCMæ ¼å¼ï¼Œæ£€æŸ¥é‡‡æ ·å€¼èŒƒå›´
            if audio_format == 'pcm' and len(audio_data) >= 2:
                import struct
                # è¯»å–å‰å‡ ä¸ª16bité‡‡æ ·å€¼
                samples = []
                for i in range(0, min(20, len(audio_data) - 1), 2):
                    sample = struct.unpack('<h', audio_data[i:i+2])[0]  # å°ç«¯åº16bitæœ‰ç¬¦å·æ•´æ•°
                    samples.append(sample)
                self.logger.stt.info(f"PCM samples (first 10): {samples[:10]}")
                
                # æ£€æŸ¥é‡‡æ ·å€¼æ˜¯å¦åˆç†
                max_sample = max(abs(s) for s in samples)
                self.logger.stt.info(f"Max sample amplitude: {max_sample} / 32768")
        
        try:
            # å°†éŸ³é¢‘æ•°æ®ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ä»¥ä¾›DashScope SDKä½¿ç”¨
            import tempfile
            import os
            
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ ¼å¼ä¿å­˜æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_suffix) as temp_file:
                temp_filename = temp_file.name
                temp_file.write(audio_data)
            
            try:
                # è®°å½•å®¢æˆ·ç«¯æ¶ˆæ¯å‘é€
                self.logger.stt.info('STT recognition request sent', 
                                      {'audio_size': len(audio_data), 'format': audio_format, 'model': self.stt_model})
                
                # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼å‚æ•°è°ƒç”¨DashScope SDK
                # paraformer-realtime-v2 æ”¯æŒ: wav, opus, mp3 (ä¸æ”¯æŒè£¸pcm)
                recognition = Recognition(
                    model=self.stt_model,
                    format=audio_format,  # ä½¿ç”¨è½¬æ¢åçš„æ ¼å¼
                    sample_rate=16000,    # 16kHzé‡‡æ ·ç‡ï¼ˆå¿…é¡»ï¼‰
                    callback=SimpleRecognitionCallback()
                )
                
                response = recognition.call(file=temp_filename)
                
                if response.status_code == 200:
                    # æå–è¯†åˆ«ç»“æœ
                    full_text = ""
                    response_str = str(response)
                    
                    # ä»å“åº”å­—ç¬¦ä¸²ä¸­æå–æ–‡æœ¬
                    import re
                    # æŸ¥æ‰¾'text'å­—æ®µçš„å€¼ï¼Œç‰¹åˆ«å…³æ³¨åŒ…å«ä¸­æ–‡çš„æ–‡æœ¬
                    text_pattern = r"'text':\s*'([^']*(?:æ¬¢è¿|åšç‰©é¦†|æ™ºèƒ½|åŠ©æ‰‹|ç«¯åˆ°ç«¯|æµ‹è¯•)[^']*)'"
                    text_match = re.search(text_pattern, response_str)
                    
                    if text_match:
                        full_text = text_match.group(1)
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ¨¡å¼
                        # å¯»æ‰¾åŒ…å«ä¸­æ–‡å¥å­çš„æ¨¡å¼
                        chinese_sentence_pattern = r"'text':\s*'([^']*(?:[\u4e00-\u9fff]){5,}[^']*)'"
                        sentence_match = re.search(chinese_sentence_pattern, response_str)
                        if sentence_match:
                            full_text = sentence_match.group(1)
                        else:
                            # æœ€åå°è¯•æå–æ‰€æœ‰ä¸­æ–‡å­—ç¬¦
                            chinese_chars = re.findall(r'[\u4e00-\u9fff]+', response_str)
                            if chinese_chars:
                                full_text = ''.join(chinese_chars)[:100]  # å–å‰100ä¸ªå­—ç¬¦
                
                    # è®°å½•å®¢æˆ·ç«¯æ¶ˆæ¯æ¥æ”¶
                    self.logger.stt.info('STT recognition response received', 
                                          {'recognized_text': full_text[:100]})
                    
                    self.logger.stt.info(f"STTè¯†åˆ«å®Œæˆ: {full_text}")
                    return full_text
                else:
                    self.logger.sys.error(f"STTè¯†åˆ«å¤±è´¥: {response.code}, {response.message}")
                    self.logger.stt.error(f'STT recognition failed', 
                                  {'error_code': response.code, 'message': response.message})
                    # è¿”å›é™çº§ç»“æœ
                    return "æµ‹è¯•éŸ³é¢‘è¯†åˆ«å†…å®¹ï¼Œå®é™…ç¯å¢ƒä¸­éœ€è¦æ­£ç¡®é…ç½®é˜¿é‡Œäº‘STTæœåŠ¡"
                    
            except Exception as e:
                self.logger.stt.error(f"STT recognition exception: {str(e)}")
                self.logger.stt.error(f'STT recognition exception', {'error': str(e)})
                # è¿”å›é™çº§ç»“æœ
                return "æµ‹è¯•éŸ³é¢‘è¯†åˆ«å†…å®¹ï¼Œå®é™…ç¯å¢ƒä¸­éœ€è¦æ­£ç¡®é…ç½®é˜¿é‡Œäº‘STTæœåŠ¡"
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_filename):
                    os.unlink(temp_filename)
            
        except Exception as e:
            self.logger.stt.error("STT recognition failed", {
                'error': str(e),
                'flow': 'RECOGNITION_ERROR'
            })
            return ""
    
    async def stream_recognize(self, audio_generator, audio_format: str = 'pcm') -> str:
        """
        çœŸæ­£çš„æµå¼è¯­éŸ³è¯†åˆ«ï¼ˆè¾“å…¥æµå¼ï¼Œè¾“å‡ºå®Œæ•´æ–‡æœ¬ï¼‰
        
        Args:
            audio_generator: éŸ³é¢‘æ•°æ®ç”Ÿæˆå™¨ï¼ˆå¼‚æ­¥è¿­ä»£å™¨ï¼‰
            audio_format: éŸ³é¢‘æ ¼å¼ï¼ˆ'pcm', 'wav', 'opus'ç­‰ï¼‰
        
        Returns:
            å®Œæ•´çš„è¯†åˆ«æ–‡æœ¬ï¼ˆç”¨äºåç»­çš„è¯­ä¹‰æ£€ç´¢ï¼‰
        """
        self.logger.stt.info('Starting real-time streaming STT', {'format': audio_format})
        
        # ç¡®ä¿é…ç½®å·²åŠ è½½
        self._ensure_config_loaded()
        
        try:
            import queue
            import threading
            
            # åˆ›å»ºé˜Ÿåˆ—ç”¨äºæ”¶é›†è¯†åˆ«ç»“æœ
            result_queue = queue.Queue()
            recognition_complete = threading.Event()
            recognition_error = [None]
            
            # å®šä¹‰ callback ç±»æ¥æ¥æ”¶å®æ—¶è¯†åˆ«ç»“æœ
            class StreamingRecognitionCallback(RecognitionCallback):
                def __init__(self, logger):
                    super().__init__()
                    self.logger = logger
                    self.partial_results = []
                    self.final_result = ""
                
                def on_open(self):
                    self.logger.stt.debug('STT WebSocket connection opened')
                
                def on_complete(self):
                    self.logger.stt.info('STT recognition completed', {'final_text': self.final_result[:100]})
                    result_queue.put(('complete', self.final_result))
                    recognition_complete.set()
                
                def on_error(self, message=None):
                    error_msg = str(message) if message else 'Unknown error'
                    self.logger.stt.error('STT recognition error', {'error': error_msg})
                    recognition_error[0] = error_msg
                    result_queue.put(('error', error_msg))
                    recognition_complete.set()
                
                def on_close(self):
                    self.logger.stt.debug('STT WebSocket connection closed')
                
                def on_event(self, result):
                    """å®æ—¶æ¥æ”¶è¯†åˆ«ç»“æœ"""
                    if result:
                        # è·å–è¯†åˆ«æ–‡æœ¬
                        sentence = result.get_sentence()
                        if sentence:
                            text = sentence.get('text', '')
                            if text:
                                # åˆ¤æ–­æ˜¯éƒ¨åˆ†ç»“æœè¿˜æ˜¯æœ€ç»ˆç»“æœ
                                is_final = sentence.get('end_time', 0) > 0
                                
                                if is_final:
                                    self.final_result = text
                                    self.logger.stt.info('STT final result', {'text': text[:100]})
                                else:
                                    self.partial_results.append(text)
                                    self.logger.stt.debug('STT partial result', {'text': text[:50]})
                                
                                result_queue.put(('partial' if not is_final else 'final', text))
            
            # åˆ›å»º callback å®ä¾‹
            callback = StreamingRecognitionCallback(self.logger)
            
            # åˆ›å»º Recognition å®ä¾‹
            recognition = Recognition(
                model=self.stt_model,
                format=audio_format,
                sample_rate=16000,
                callback=callback
            )
                
            # å¯åŠ¨æµå¼è¯†åˆ«
            recognition.start()
            self.logger.stt.info('STT streaming started')
            
            # åœ¨åå°çº¿ç¨‹å‘é€éŸ³é¢‘æ•°æ®
            async def send_audio_frames():
                try:
                    frame_count = 0
                    async for audio_chunk in audio_generator:
                        if audio_chunk:
                            # å‘é€éŸ³é¢‘å¸§
                            recognition.send_audio_frame(audio_chunk)
                            frame_count += 1
                            self.logger.stt.debug('Sent audio frame', {
                                'frame_num': frame_count,
                                'size': len(audio_chunk)
                            })
                    
                    # æ‰€æœ‰éŸ³é¢‘å‘é€å®Œæ¯•ï¼Œåœæ­¢è¯†åˆ«
                    recognition.stop()
                    self.logger.stt.info('STT streaming stopped', {'total_frames': frame_count})
                    
                except Exception as e:
                    recognition_error[0] = str(e)
                    self.logger.stt.error('Error sending audio frames', {'error': str(e)})
                    recognition_complete.set()
            
            # å¯åŠ¨éŸ³é¢‘å‘é€ä»»åŠ¡
            send_task = asyncio.create_task(send_audio_frames())
            
            # ç­‰å¾…è¯†åˆ«å®Œæˆ
            while not recognition_complete.is_set():
                await asyncio.sleep(0.1)
            
            # ç­‰å¾…å‘é€ä»»åŠ¡å®Œæˆ
            await send_task
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if recognition_error[0]:
                raise Exception(f'STT recognition failed: {recognition_error[0]}')
            
            # è¿”å›å®Œæ•´çš„è¯†åˆ«æ–‡æœ¬
            final_text = callback.final_result
            self.logger.stt.info('Real-time streaming STT completed', {'final_text': final_text[:100]})
            
            return final_text
        
        except Exception as e:
            self.logger.stt.error('Streaming STT recognition failed', {'error': str(e)})
            raise e


# ä¿ç•™å‘åå…¼å®¹çš„åˆ«å
STTService = UnifiedSTTService